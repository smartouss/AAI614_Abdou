{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEpfj0kDG2V2"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import ssl\n",
        "\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "for link in bs.find_all('a'):\n",
        "    if 'href' in link.attrs:\n",
        "        print(link.attrs['href'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq60YzoWG2V2"
      },
      "source": [
        "## Retrieving Articles Only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOHfTRQ7G2V4"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "for link in bs.find('div', {'id':'bodyContent'}).find_all(\n",
        "    'a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
        "    print(link.attrs['href'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enm_3SsvG2V4"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
        "bs = BeautifulSoup(html, 'html.parser')\n",
        "for link in bs.find('div', {'id':'bodyContent'}).find_all(\n",
        "    'a', href=re.compile('^(/wiki/)((?!:).)*$')):\n",
        "    print(link.attrs['href'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeUFJmJOG2V4"
      },
      "source": [
        "## Random Walk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s53AOcAG2V4"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import random\n",
        "import re\n",
        "\n",
        "random.seed(datetime.datetime.now().strftime('%s'))\n",
        "def getLinks(articleUrl):\n",
        "    html = urlopen(f'http://en.wikipedia.org{articleUrl}')\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    return bs.find('div', {'id':'bodyContent'}).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$'))\n",
        "\n",
        "links = getLinks('/wiki/Kevin_Bacon')\n",
        "while len(links) > 0:\n",
        "    newArticle = links[random.randint(0, len(links)-1)].attrs['href']\n",
        "    print(newArticle)\n",
        "    links = getLinks(newArticle)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4OKHotIG2V5"
      },
      "source": [
        "## Recursively crawling an entire site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zedXlR9pG2V5"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "pages = set()\n",
        "def getLinks(pageUrl):\n",
        "    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
        "        if 'href' in link.attrs:\n",
        "            if link.attrs['href'] not in pages:\n",
        "                #We have encountered a new page\n",
        "                newPage = link.attrs['href']\n",
        "                print(newPage)\n",
        "                pages.add(newPage)\n",
        "                getLinks(newPage)\n",
        "getLinks('')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEg7qau4G2V5"
      },
      "source": [
        "## Collecting Data Across an Entire Site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjuH9MWxG2V5"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "pages = set()\n",
        "def getLinks(pageUrl):\n",
        "    html = urlopen(f'http://en.wikipedia.org{pageUrl}')\n",
        "    bs = BeautifulSoup(html, 'html.parser')\n",
        "    try:\n",
        "        print(bs.h1.get_text())\n",
        "        #mw-parser-output\n",
        "        bodyContent = bs.find('div', {'id':'bodyContent'}).find_all('p')\n",
        "        if len(bodyContent):\n",
        "            print(bodyContent[0])\n",
        "        print(bs.find(id='ca-edit').find('a').attrs['href'])\n",
        "    except AttributeError:\n",
        "        print('This page is missing something! Continuing.')\n",
        "\n",
        "    for link in bs.find_all('a', href=re.compile('^(/wiki/)')):\n",
        "        if 'href' in link.attrs:\n",
        "            if link.attrs['href'] not in pages:\n",
        "                #We have encountered a new page\n",
        "                newPage = link.attrs['href']\n",
        "                print('-'*20)\n",
        "                print(newPage)\n",
        "                pages.add(newPage)\n",
        "                getLinks(newPage)\n",
        "getLinks('/wiki/General-purpose_programming_language')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENWcU8stG2V5"
      },
      "source": [
        "## Crawling across the Internet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzbcq15DG2V5"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from urllib.parse import urlparse\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import datetime\n",
        "import random\n",
        "\n",
        "#Retrieves a list of all Internal links found on a page\n",
        "def getInternalLinks(bs, url):\n",
        "    netloc = urlparse(url).netloc\n",
        "    scheme = urlparse(url).scheme\n",
        "    internalLinks = set()\n",
        "    for link in bs.find_all('a'):\n",
        "        if not link.attrs.get('href'):\n",
        "            continue\n",
        "        parsed = urlparse(link.attrs['href'])\n",
        "        if parsed.netloc == '':\n",
        "            internalLinks.add(f'{scheme}://{netloc}/{link.attrs[\"href\"].strip(\"/\")}')\n",
        "        elif parsed.netloc == netloc:\n",
        "            internalLinks.add(link.attrs['href'])\n",
        "    return list(internalLinks)\n",
        "\n",
        "#Retrieves a list of all external links found on a page\n",
        "def getExternalLinks(bs, url):\n",
        "    netloc = urlparse(url).netloc\n",
        "    externalLinks = set()\n",
        "    for link in bs.find_all('a'):\n",
        "        if not link.attrs.get('href'):\n",
        "            continue\n",
        "        parsed = urlparse(link.attrs['href'])\n",
        "        if parsed.netloc != '' and parsed.netloc != netloc:\n",
        "            externalLinks.add(link.attrs['href'])\n",
        "    return list(externalLinks)\n",
        "\n",
        "def getRandomExternalLink(startingPage):\n",
        "    bs = BeautifulSoup(urlopen(startingPage), 'html.parser')\n",
        "    externalLinks = getExternalLinks(bs, startingPage)\n",
        "    if not len(externalLinks):\n",
        "        print('No external links, looking around the site for one')\n",
        "        internalLinks = getInternalLinks(bs, startingPage)\n",
        "        return getRandomExternalLink(random.choice(internalLinks))\n",
        "    else:\n",
        "        return random.choice(externalLinks)\n",
        "\n",
        "def followExternalOnly(startingSite):\n",
        "    externalLink = getRandomExternalLink(startingSite)\n",
        "    print(f'Random external link is: {externalLink}')\n",
        "    followExternalOnly(externalLink)\n",
        "\n",
        "\n",
        "followExternalOnly('https://www.oreilly.com/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kESd3VXgG2V6"
      },
      "source": [
        "## Collect all External Links from a Site"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5csQJ_VG2V6"
      },
      "outputs": [],
      "source": [
        "# Collects a list of all external URLs found on the site\n",
        "allExtLinks = []\n",
        "allIntLinks = []\n",
        "\n",
        "\n",
        "def getAllExternalLinks(url):\n",
        "    bs = BeautifulSoup(urlopen(url), 'html.parser')\n",
        "    internalLinks = getInternalLinks(bs, url)\n",
        "    externalLinks = getExternalLinks(bs, url)\n",
        "    for link in externalLinks:\n",
        "        if link not in allExtLinks:\n",
        "            allExtLinks.append(link)\n",
        "            print(link)\n",
        "\n",
        "    for link in internalLinks:\n",
        "        if link not in allIntLinks:\n",
        "            allIntLinks.append(link)\n",
        "            getAllExternalLinks(link)\n",
        "\n",
        "\n",
        "allIntLinks.append('https://oreilly.com')\n",
        "getAllExternalLinks('https://www.oreilly.com/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ii1bB7TFG2V6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
